{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea11d17-fe37-4e33-91fa-88fe592d8fa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deducing Heart Disease in Patients #\n",
    "\n",
    "#### Ali Abbas, Jayson Wu, Ria Perencsik, and Durrotul Salma ####\n",
    "\n",
    "## Introduction ##\n",
    "\n",
    "#### __Background Information__ ####\n",
    "Heart disease describes a range of conditions that affect the heart, such as coronary heart disease and cardiomyopathy. Heart disease affects millions globally and is a major health concern, taking an estimated 17.9 million lives each year. One third of these deaths occur prematurely in people under 70 years of age. Many factors like high blood pressure and high cholesterol are associated with an increased risk of heart disease. \n",
    "\n",
    "The importance of identifying heart disease in patients early cannot be overstated. <br> As such, this project aims to assess the efficacy of predicting a patient's likelihood of having heart disease based on two key factors: \n",
    " - __ST depression__ (Note: ST corresponds to a segment seen in an ECG test. If the segment appears abnormally low and sits below the baseline, the person is said to have ST depression)\n",
    " - __maximum heart rate__\n",
    "\n",
    "to answer the question: <br>\n",
    "*How accurately can the presence of heart disease be predicted in patients through the classification of ST depression induced by exercise relative to rest and maximum heart rate as key factors in the assessment?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f90bd-aab7-49bb-90bd-c7d20c88e818",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### __Datasets Used__ ####\n",
    "We utilized the datasets: 'processed.cleveland.data', 'reprocessed.hungarian.data', 'processed.switzerland.data', and 'processed.va.data' from the Heart Disease Database to answer this question.\n",
    "\n",
    "<br> The column names created by the dataset are as follows: \n",
    "\n",
    "1. **Age** - Patient's age\n",
    "2. **Sex** - Patient's sex\n",
    "3. **Chest_Pain_Type** - Patient's chest pain type\n",
    "4. **Resting_Blood_Pressure** - Patient's Resting blood pressure\n",
    "5. **Cholesterol** - Patient's serum cholesterol level (mg/dl)\n",
    "6. **Fasting_blood_sugar_over_120_mg/dl** - True if patient's fasting blood sugar > 120mg/dl\n",
    "7. **Resting_ecg_results** - Patient's resting electrocardiographic results\n",
    "8. **Max_heart_rate** - Patient's maximum heart rate\n",
    "9. **Exercise_induced_angina** - 'yes' if patient's exercise induced angina, 'no' otherwise\n",
    "10. **ST_depression_induced_by_exercise_relative_to_rest** - Patient's ST Depression induced by exercise relative to rest levels\n",
    "11. **slope_of_the_peak_exercise_ST_segment** - The slope of the peak exercise ST segment ('downsloping', 'flat', or 'upsloping')\n",
    "12. **Number_of_major_vessels_colored_by_flourosopy** - Number of major vessels (0-3) colored by flourosopy\n",
    "13. **Thalassemia** - Presence of thalassemia in a patient ('fixed', 'reversible', or 'normal')\n",
    "14. **Class** - Diagnosis of heart disease ('healthy' or 'sick')\n",
    "\n",
    "\n",
    "Among these variables, ST depression and maximum heart rate were selected as predictors for the classification analysis due to their superior ability to form distinct clusters between healthy and sick groups compared to other factors in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0023a-42b7-4e2a-9090-7fe9e53b0317",
   "metadata": {},
   "source": [
    "As mentioned in `worksheet_classification2.ipynb`, we are to call `set.seed` exactly once at the beginning of the analysis, so that our random numbers are actually reasonably random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd7d76-2cc1-47b5-a03e-3c22ff6699bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c12d9-739e-4c62-bfa6-0b318c176b9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading the data set from the web into R ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255247d7-c97e-4724-899f-3751399af96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run this cell before continuing. \n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(readxl)\n",
    "library(tidymodels)\n",
    "# library(rvest)\n",
    "# library(stringr)\n",
    "# install.packages('janitor')\n",
    "# library(janitor)\n",
    "library(ggplot2)\n",
    "install.packages(\"kknn\")\n",
    "library(kknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff824014-411b-49e7-93b0-8dbb7b4e76de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading a csv file containing the data in processed.cleveland.data, \n",
    "# with a row of column names (these names are essentially the column names specified in cleve.mod, under ‘Original atts’, without the stuff in brackets)\n",
    "options(repr.matrix.max.rows = 25)\n",
    "\n",
    "cleveland_dataset_web <- read_csv(file = url(\"https://archive.ics.uci.edu/static/public/45/data.csv\"))\n",
    "\n",
    "# extracting the column names from the very first line of the csv file (because the other files I plan to import do not contain column names)\n",
    "first_row_contents <- read_csv(file = url(\"https://archive.ics.uci.edu/static/public/45/data.csv\"), n_max = 1) |> names()\n",
    "\n",
    "# reading a file containing the data in processed.switzerland.data\n",
    "switzerland_dataset_web <- read_delim(file = url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data\"), \n",
    "                                    delim = \",\", col_names = first_row_contents) |>\n",
    "                        mutate(trestbps = as.numeric(trestbps), chol = as.numeric(chol), fbs = as.numeric(fbs), restecg = as.numeric(restecg), \n",
    "                               thalach = as.numeric(thalach), exang = as.numeric(exang), oldpeak = as.numeric(oldpeak),\n",
    "                               slope = as.numeric(slope), ca = as.numeric(ca), thal = as.numeric(thal))\n",
    "\n",
    "# reading a file containing the data in processed.va.data\n",
    "virginia_dataset_web <- read_delim(file = url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data\"), \n",
    "                                    delim = \",\", col_names = first_row_contents) |>\n",
    "                        mutate(trestbps = as.numeric(trestbps), chol = as.numeric(chol), fbs = as.numeric(fbs), \n",
    "                               thalach = as.numeric(thalach), exang = as.numeric(exang), oldpeak = as.numeric(oldpeak),\n",
    "                               slope = as.numeric(slope), ca = as.numeric(ca), thal = as.numeric(thal))\n",
    "\n",
    "# reading a file containing the data in reprocessed.hungarian.data\n",
    "reprocessed_hungarian_dataset_web <- read_delim(file = url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/reprocessed.hungarian.data\"), \n",
    "                                    delim = \" \", col_names = first_row_contents) |>\n",
    "                        mutate(trestbps = as.numeric(trestbps), chol = as.numeric(chol), fbs = as.numeric(fbs), restecg = as.numeric(restecg), \n",
    "                               thalach = as.numeric(thalach), exang = as.numeric(exang), oldpeak = as.numeric(oldpeak),\n",
    "                               slope = as.numeric(slope), ca = as.numeric(ca), thal = as.numeric(thal))\n",
    "\n",
    "# reading the file titled heart-disease.names, because I will reference information when justifying some of the decisions we made as far as tidying the data is concerned\n",
    "notes_about_data <- read_file(file = url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names\"))\n",
    "\n",
    "global_dataset_missing <- bind_rows(cleveland_dataset_web, switzerland_dataset_web, virginia_dataset_web, reprocessed_hungarian_dataset_web)\n",
    "\n",
    "global_dataset_missing_first_25_rows <- global_dataset_missing |> slice(1:25)\n",
    "# global_dataset_missing_first_25_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922be75-681b-4e9c-aa1b-36a02eacb89f",
   "metadata": {},
   "source": [
    "Here we convert the value -9 into NA, because of this excerpt from the file named heart-disease.names: \n",
    "> \"Missing Attribute Values: Several.  Distinguished with value -9.0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c2684-141f-4867-b73a-0dc3d5b3cd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "na_matrix <- global_dataset_missing == \"-9\"\n",
    "\n",
    "is.na(global_dataset_missing) <- na_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc05d4-2fc9-40ae-9098-a047a8dcd0f3",
   "metadata": {},
   "source": [
    "# Cleaning and wrangling the data into a tidy format #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226b0b7-9719-4b52-a1bb-2404b500871f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) summarizing the data in at least one table using only training data: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853a88d-1c55-415c-96b3-de710de6ea9e",
   "metadata": {},
   "source": [
    "we now try to create\n",
    "a table that reports \n",
    "the number of healthy and sick observations,\n",
    "the number of rows with missing values for healthy and sick observations,\n",
    "the percentage of healthy and sick observations,\n",
    "the average ages, resting blood pressures, cholesterol,\n",
    " max heart rate, ST depression induced by exercise relative to rest,\n",
    "and the average number of vessels colored by flourosopy for healthy and sick observations\n",
    "for each class in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39973-ae8d-46a2-b1d1-425208e5a8f5",
   "metadata": {},
   "source": [
    "The description of the num attribute in heart-disease.names doesn't match the possible values of num in any of the 4 data files we're using in our analysis.\n",
    "\n",
    "here are the discrepancies:\n",
    "\n",
    "a) under the subsection of heart-disease.names titled \"7. Attribute Information\", \n",
    "it states that num can either be 0 or 1, but looking through our data, values of num in the range 0:4 can be found within the first 25 rows of global_dataset\n",
    "\n",
    "b) under the subsection of heart-disease.names titled \"7. Attribute Information\", it is also stated that 0 indicates < 50% diameter narrowing,\n",
    "while 1 indicates > 50% diameter narrowing. However, it contradicts the description of num under the subsection of heart-disease.names titled \"4. Relevant Information\",\n",
    "which states that \"Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0)\".\n",
    " \n",
    "As such, we strongly believe that 0 does not indicate < 50% diameter narrowing, and 1 does not indicate > 50% diameter narrowing.\n",
    "\n",
    "We proceed with our analysis based on the understanding that value 0 indicates absence of heart disease, while values 1, 2, 3, 4 indicate presence of heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef39e9-330c-4e6c-99e2-d57f0d447175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_dataset_missing <- as_tibble(global_dataset_missing)\n",
    "\n",
    "global_dataset_missing |>\n",
    "      rename(Class = num) |>\n",
    "      mutate(Class = as.factor(Class)) |>\n",
    "      mutate(Class = fct_recode(Class, \"healthy\" = \"0\", \"sick\" = \"1\", \"sick\" = \"2\", \"sick\" = \"3\", \"sick\" = \"4\")) |>\n",
    "      mutate(row_contains_na = (is.na(age) | is.na(sex) | is.na(cp) | is.na(trestbps) | is.na(chol) | is.na(fbs) | is.na(restecg) | is.na(thalach) | is.na(exang) | is.na(oldpeak) | is.na(slope) | is.na(ca) | is.na(thal))) |>\n",
    "      group_by(Class) |>\n",
    "      summarize(\n",
    "         count = n(), \n",
    "         num_rows_with_na = sum(row_contains_na),\n",
    "         percentage = count / nrow(global_dataset_missing) * 100,\n",
    "         average_age = mean(age, na.rm = TRUE),\n",
    "         avg_resting_bp = mean(trestbps, na.rm = TRUE),\n",
    "         avg_cholestorol = mean(chol, na.rm = TRUE),\n",
    "         avg_max_hr = mean(thalach, na.rm = TRUE),\n",
    "         avg_oldpeak = mean(oldpeak, na.rm = TRUE),\n",
    "         avg_ca = mean(ca, na.rm = TRUE)\n",
    "         )\n",
    "\n",
    "global_dataset_missing |>\n",
    "      pivot_longer(cols = c(age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal)) |>\n",
    "      rename(Class = num) |>\n",
    "      mutate(Class = as.factor(Class)) |>\n",
    "      mutate(Class = fct_recode(Class, \"healthy\" = \"0\", \"sick\" = \"1\", \"sick\" = \"2\", \"sick\" = \"3\", \"sick\" = \"4\")) |>\n",
    "      group_by(Class, name) |>\n",
    "      summarize(\n",
    "         num_missing_values = sum(is.na(value))\n",
    "      ) |> \n",
    "      group_by(Class) |>\n",
    "      summarize(num_cols_with_na = sum(num_missing_values > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9824a4-1c17-4483-bdee-c831bd548644",
   "metadata": {
    "tags": []
   },
   "source": [
    "**ADD TITLE FOR THESE TWO TABLES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb41a1-7c5a-49c0-ba4a-5c4f7e40ae10",
   "metadata": {},
   "source": [
    "Explaining what the values of num_rows_with_na (third column of the first summary tibble) and num_cols_with_na (second column of the second summary table) represent:\n",
    "\n",
    "The third column of the first summary tibble conveys that, out of all the rows in the dataset, 439 of the rows corresponding to healthy observations have NA values, while 475 of rows corresponding to sick observations have NA values. Meanwhile, the second column of the second summary tibble conveys that, out of all the rows in the dataset, the NA values in the rows corresponding to healthy observations are all located in exactly 9 of the 14 columns while the NA values in the rows corresponding to sick observations are also located in exactly 10 of the 14 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811c40c-10da-4b47-b505-acb833fd1a36",
   "metadata": {},
   "source": [
    "# Justification for the datasets we chose to use: #\n",
    "\n",
    "After reading through lines 109 - 124 of **heart-disease.names**, \n",
    "it became clear to us that, although the data collected during numerous heart disease diagnoses could be partitioned into 76 attributes, only 14 of the attributes were eventually used in experiments revolving heart diseases. Since the objective of our project is to predict whether or not an observed individual may or may not have heart disease, we decided that we would also choose from the data files that consisted of only the 14 attributes that had been used by researchers.\n",
    "As such, our choices for data files were narrowed down to:\n",
    "> processed.cleveland.data\n",
    ">\n",
    "> data.csv \n",
    ">\n",
    "> processed.hungarian.data\n",
    ">\n",
    "> processed.switzerland.data\n",
    ">\n",
    "> processed.va.data\n",
    ">\n",
    "> reprocessed.hungarian.data\n",
    "\n",
    "Deciding between processed.cleveland.data and data.csv:\n",
    "> we saw that data.csv essentially contained the same information as processed.cleveland.data, \n",
    "> but data.csv also contained the names of the 14 attributes in the correct order. Therefore, \n",
    "> we decided to go with data.csv over processed.cleveland.data\n",
    "\n",
    "Deciding between processed.hungarian.data and reprocessed.hungarian.data\n",
    "> we saw that both files were identical save for the fact that the missing values in processed.hungarian.data were represented with a \"?\".\n",
    "> As such, we decided to go with reprocessed.hungarian.data over processed.hungarian.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af767e-8a7e-44e6-8822-5d1f5ca941cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparing the dataset for the code that generates visualizations\n",
    "\n",
    "# here I am merely moving the num column from the rightmost column to the leftmost column\n",
    "global_dataset_missing <- global_dataset_missing |>\n",
    "      select(num, age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ab65c-219c-4e18-8faa-f26ac4603887",
   "metadata": {},
   "source": [
    "Since a large proportion of rows in the dataset associated with both healthy and sick observations, respectively, have NA values, we have made the decision to impute the missing entries of each row in the dataset. \n",
    "\n",
    "\n",
    "I had to drop all categorical variables, which includes Sex, Chest Pain Type (cp), Fasting_blood_sugar_over_120_mg/dl (fbs), Resting_ecg_results (restecg), Exercise_induced_angina (exang), slope_of_the_peak_exercise_ST_segment (slope), and Thalassemia type (thal). \n",
    "\n",
    "The reason is that those are categorical variables, and even if they're encoded by doubles, it does not make sense to impute missing values of these variables. \n",
    "In addition, although the Number_of_major_vessels_colored_by_flourosopy (ca) looks like a numerical variable, it is actually a categorical variable, because the set of possible values that this variable can take on is {0, 1, 2, 3}. As such, this variable has also been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9e0e6-2ede-4a37-b744-d1ebfb2d463a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows = 10)\n",
    "\n",
    "global_dataset_missing <- global_dataset_missing |>\n",
    "      rename(Class = num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1fe99f-0247-47ba-9eec-994aa6a45099",
   "metadata": {},
   "source": [
    "Creating a tidymodels recipe to standardize our predictors and impute their missing entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406129e-c754-4578-8bee-8268e5d8b4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "impute_missing_recipe <- recipe(Class ~ age + trestbps + chol + thalach + oldpeak, data = global_dataset_missing) |>\n",
    "                         step_impute_mean(all_predictors()) |>\n",
    "                         step_center(all_predictors()) |>\n",
    "                         step_scale(all_predictors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc76716-8481-4acc-a47c-d6ef42d653ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputed_and_scaled_global_dataset <- impute_missing_recipe |>  \n",
    "                            prep() |> \n",
    "                            bake(global_dataset_missing)\n",
    "imputed_and_scaled_global_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc938d8-7d35-40ae-9ae0-c8a56c14124c",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e29ca-df24-46a6-b375-d0d7b6101bec",
   "metadata": {},
   "source": [
    "here I am merely moving the Class column from the rightmost column to the leftmost column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd37a4-cbfb-4f33-934c-2e02d320ebd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputed_and_scaled_global_dataset <- imputed_and_scaled_global_dataset |>\n",
    "      select(Class, age, trestbps, chol, thalach, oldpeak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61551da7-c75e-4fdc-9e49-f18691598414",
   "metadata": {},
   "source": [
    "## 2) visualizing the data with a plot relevant to the analysis we plan to do using only training data: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775d6e-7c19-481e-9cce-aa730caf33bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputed_and_scaled_global_dataset <- imputed_and_scaled_global_dataset |>\n",
    "\n",
    "      mutate(Class = as.factor(Class)) |>\n",
    "      mutate(Class = fct_recode(Class, \"healthy\" = \"0\", \"sick\" = \"1\", \"sick\" = \"2\", \"sick\" = \"3\", \"sick\" = \"4\"))\n",
    "\n",
    "imputed_and_scaled_global_dataset <- imputed_and_scaled_global_dataset |>\n",
    "      rename(Age = age, \"Resting_Blood_Pressure\" = trestbps, Cholesterol = chol, \"Max_heart_rate\" = thalach, \n",
    "      \"ST_depression_induced_by_exercise_relative_to_rest\" = oldpeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9dc29-5d86-4d72-8836-2cc6188c0a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 18, repr.plot.height = 18) \n",
    "\n",
    "# coloured and grouped-by-shape scatterplot\n",
    "\n",
    "# Max_heart_rate & ST_depression_induced_by_exercise_relative_to_rest\n",
    "mhr_oldpeak <- imputed_and_scaled_global_dataset |> \n",
    "    ggplot(aes(x = Max_heart_rate, y = ST_depression_induced_by_exercise_relative_to_rest)) + \n",
    "    geom_point(aes(colour = Class, shape = Class), size = 3) + \n",
    "    labs(title = \"Max Heart Rate versus ST Depression induced by exercise relative to rest\", x = \"Max Heart Rate\", y = \"ST Depression induced by exercise relative to rest\", colour = \"Heart Disease Diagnosis\", shape = \"Heart Disease Diagnosis\") + \n",
    "    theme(text = element_text(size = 20), plot.title = element_text(face = \"bold\"))\n",
    "# mhr_oldpeak\n",
    "\n",
    "# Justification for why we decided to use scatterplots (with all possible pairings of numerical variables) to pick our predictor variables:\n",
    "\n",
    "# 1) We wanted to look for predictor variables from the set of numerical variables in our dataset, because we plan on using the KNN classification algorithm, \n",
    "# which is known to yield more accurate predictions when the predictor variables are numerical instead of categorical\n",
    "\n",
    "# 2) We created scatterplots (where for each distinct value of a class, the observations with that class value have a unique colour and shape) \n",
    "# for every possible pairing of numerical variables\n",
    "# because we thought that \n",
    "# by making scatterplots for each possible pairing of numerical variables \n",
    "# and comparing each of these plots to see which one shows the most apparent separation of the observations belonging to each of the distinct classes,\n",
    "# we'd be able to determine the best pair of predictor variables for KNN.\n",
    "# These variables would be the ones plotted on the x-axis and y-axis of the scatterplot with the clearest separation of the distinct classes.\n",
    "\n",
    "# In our case, since we felt that the plot of \"Max Heart Rate\" against \"ST depression induced by exercise relative to rest\" was the one that did the best job at showing the separation \n",
    "# of observations classified as 'healthy' and observations classified as 'sick', we came to the conclusion that the two aformentioned variables would be the most suitable predictor variables for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ced0ef-e1e8-424d-b795-f88bdaf23a9b",
   "metadata": {},
   "source": [
    "Before we begin the classification process, we will make a copy of global_dataset_missing and name it 'heart_data'.\n",
    "This is because we want to keep our code as concise as possible.\n",
    "We will then clean heart_data so that it will contain the same columns and rows as imputed_and_scaled_global_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb36f08-8c0b-4c4e-8d8e-fdd41ed1a9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_data <- global_dataset_missing\n",
    "\n",
    "heart_data <- heart_data |>\n",
    "      select(Class, age, trestbps, chol, thalach, oldpeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30a545-e862-4357-b5f4-14380cd2dba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_data <- heart_data |>\n",
    "\n",
    "      mutate(Class = as.factor(Class)) |>\n",
    "      mutate(Class = fct_recode(Class, \"healthy\" = \"0\", \"sick\" = \"1\", \"sick\" = \"2\", \"sick\" = \"3\", \"sick\" = \"4\"))\n",
    "\n",
    "heart_data <- heart_data |>\n",
    "      rename(Age = age, \"Resting_Blood_Pressure\" = trestbps, Cholesterol = chol, \"Max_heart_rate\" = thalach, \n",
    "      \"ST_depression_induced_by_exercise_relative_to_rest\" = oldpeak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71271a2-0c3a-473b-b4a8-b794df2875b0",
   "metadata": {},
   "source": [
    "Since we plan to split our data into a training set and a test set, we will not be imputing the missing entries of our predictors. \n",
    "\n",
    "This is because having numerous imputed values in both our training set and our testing set will lead to misleading conclusions about the accuracy of our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3cfdeb-2c7b-44a8-b1f3-729d97c1ecdb",
   "metadata": {},
   "source": [
    "Now we begin the classification process.\n",
    "\n",
    "We will be partitioning heart_data into a training (75%) and testing (25%) set.\n",
    "After creating the test set,\n",
    "we will put the test set away in a lock box and not touch it again\n",
    "until we have found the best k-nn classifier we can make using the training set.\n",
    "\n",
    "We will use the variable `Class` as our class label.\n",
    "\n",
    "To create the training and test set, we first use the initial_split function to split heart_data.\n",
    "We specify that we want to use 75% of the data.\n",
    "For the strata argument, we pass in the variable we want to classify,\n",
    "which, in this case, is `Class`.\n",
    "The object we create will be named `heart_split`.\n",
    "\n",
    "Next, we pass the heart_split object to the training and testing functions\n",
    "and name the respective objects heart_train and heart_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8915b8-0166-4924-9c2e-36bdf648a2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_split <- initial_split(heart_data, prop = 0.75, strata = Class)\n",
    "heart_train <- training(heart_split)\n",
    "heart_test <- testing(heart_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e94e7f-db3f-4d23-8002-03f79db63491",
   "metadata": {},
   "source": [
    "Creating a tidymodels recipe \n",
    "to standardize our predictors and impute their missing entries, which are\n",
    "`Max_heart_rate` and `ST_depression_induced_by_exercise_relative_to_rest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a49527-872c-4116-875b-78941ab2d6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_recipe <- recipe(Class ~ Max_heart_rate + ST_depression_induced_by_exercise_relative_to_rest , data = heart_train) |>\n",
    "                step_impute_mean(all_predictors()) |>\n",
    "                step_scale(all_predictors()) |>\n",
    "                step_center(all_predictors())\n",
    "\n",
    "imputed_and_scaled_heart_train_preview <- heart_recipe |>\n",
    "               prep() |>\n",
    "               bake(heart_train)\n",
    "imputed_and_scaled_heart_train_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62200ca-0e23-45e1-a42b-1287bbf6c298",
   "metadata": {},
   "source": [
    "Creating a model specification for K-nearest neighbours classification\n",
    "by using the nearest_neighbor() function.\n",
    "Specify that we want to set k = 3 and use the straight-line distance.\n",
    "Furthermore, specify the computational engine to be \"kknn\"\n",
    "for training the model with the set_engine() function.\n",
    "Finally, identify that this is a classification problem with the set_mode() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6b913-167c-40a2-9692-ae04136c8dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) |>\n",
    "       set_engine(\"kknn\") |>\n",
    "       set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c99914-8340-44b1-9c04-527e98a3a0a7",
   "metadata": {},
   "source": [
    "Combining heart_recipe with knn_spec in a workflow, and fitting to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995f4d7-4479-4bc6-896b-5775bc90d7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_fit <- workflow() |>\n",
    "             add_recipe(heart_recipe) |>\n",
    "             add_model(knn_spec) |>\n",
    "             fit(data = heart_train)\n",
    "heart_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b08c5-a503-4d2a-ab53-de54f805ab17",
   "metadata": {},
   "source": [
    "Now that we have created our K-nearest neighbor classifier object,\n",
    "we can predict the class labels for our test set.\n",
    "\n",
    "First, we pass our fitted model and the test dataset to the predict function.\n",
    "Then, we use the bind_cols function to add the column of predictions to the original test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8bb59-fd24-428b-abc8-2f5c529d0678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_test_predictions <- predict(heart_fit , heart_test) |>\n",
    "                          bind_cols(heart_test)\n",
    "heart_test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af078c-5cf1-4e5a-8946-d4de47f14f2e",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a5543-084f-4f16-b865-52b887b3c38b",
   "metadata": {},
   "source": [
    "Then, we use the metrics function to get statistics about the quality of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1aba7-c081-4bee-aea5-f167a4bdc6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_model_metrics <- heart_test_predictions |> \n",
    "        metrics(truth = Class, estimate = .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f1cdd-ca58-4e9d-ab74-dfe6ff3885dc",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe5076-ae62-49d2-b2e4-70e6aea04863",
   "metadata": {},
   "source": [
    "Finally, we use the metrics function to get statistics about the quality of our model. \n",
    "\n",
    "For the first assignment, we assign the name of the column containing the true class values (of each observation) to `truth`. For the second argument, we assign the name of the column containing the predicted class values (of each observation) to `estimate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2e53a-cd12-4d7d-b6ab-f7a84f669633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_mat <- heart_test_predictions |>\n",
    "             conf_mat(truth = Class, estimate = .pred_class)\n",
    "heart_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8288151-e3fd-4225-84cc-61c25bef8601",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**\n",
    "\n",
    "\n",
    "Talk about the precision and recall calculated from this confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c363da-a29a-40c5-ba46-19e79b9833c0",
   "metadata": {},
   "source": [
    "Now we perform a cross-validation in R using the vfold_cv function. <br><br>\n",
    "\n",
    "To use this function, <br>\n",
    "we have identified the training set - heart_train, <br>\n",
    "we have specified the number of folds - 5, <br>\n",
    "and we have specified the strata argument - Class. <br>\n",
    "This code performs 5-fold cross-validation on heart_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb43a5-8c8f-4061-b752-9a1ec6881f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_vfold <- vfold_cv(heart_train, v = 5, strata = \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72cc69-abed-421e-bf3a-39872a5400d8",
   "metadata": {},
   "source": [
    "Now we perform the workflow analysis again; <br><br>\n",
    "First, we combine heart_recipe with knn_spec in a workflow, and fit to the training set. <br>\n",
    "We will reuse the heart_recipe and knn_spec objects we made earlier. <br>\n",
    "When we are fitting the knn model, <br>\n",
    "we're going to use the fit_resamples function instead of the fit function for training; <br>\n",
    "this function will allow us to run a cross-validation <br>\n",
    "on each train/validation split we created via the call to vfold_cv(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d21215-cde0-444d-9e4b-b6658085b0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_resample_fit <- workflow() |>\n",
    "                   add_recipe(heart_recipe) |>\n",
    "                   add_model(knn_spec) |>\n",
    "                   fit_resamples(resamples = heart_vfold)\n",
    "# heart_resample_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3149bd5-bf0e-4b44-bcc3-533207374d87",
   "metadata": {},
   "source": [
    "Now that we have ran a cross-validation on each train/validation split,\n",
    "we are going to establish the accuracy of the classifier's validation across the folds.\n",
    "\n",
    "We will aggregate the mean and standard error by using the collect_metrics function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25348b76-3443-4eb9-8ee2-e4f088d66c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_metrics <- heart_resample_fit |>\n",
    "              collect_metrics()\n",
    "heart_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83521c5b-e1c4-4c68-9386-fcb44bb48675",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0783f2-cae2-43a6-aaf5-2e6a9286c72e",
   "metadata": {},
   "source": [
    "We've just established a prediction accuracy for our classifier \n",
    "with the help of a 5-fold cross-validation.\n",
    "\n",
    "We are now going to improve our classifer,\n",
    "which means we have to change the K-value.\n",
    "\n",
    "In order to do so, \n",
    "we first create a model specification for k-nn classification, \n",
    "passing tune() as the second argument to nearest_neighbor(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35285dfc-4037-419a-9f99-428e1b3f3534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "            set_engine(\"kknn\") |>\n",
    "            set_mode(\"classification\")\n",
    "knn_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbce57-0262-4d5f-9c4c-dd5eb4d65e6b",
   "metadata": {},
   "source": [
    "Then we combine heart_recipe with knn_tune in a workflow, and fit to the training set.\n",
    "\n",
    "Unlike the previous workflow analyses,\n",
    "\n",
    "which would fit the model to a particular K-value \n",
    "\n",
    "that was the second argument passed to nearest_neighbor(...),\n",
    "\n",
    "we use tune_grid(...) to fit the model for each value in a range of parameter values.\n",
    "\n",
    "We pass the cross-validation heart_vfold model we created earlier as the resamples argument,\n",
    "\n",
    "and we pass 10 as the grid argument, \n",
    "\n",
    "which entails that the tuning will experiment with 10 different K-values.\n",
    "\n",
    "\n",
    "After the call to tune_grid(...), \n",
    "\n",
    "we aggregate the mean and standard error by using the collect_metrics function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdab21-326b-4e6b-ae94-8d5416252f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows = 20)\n",
    "\n",
    "knn_results <- workflow() |>\n",
    "               add_recipe(heart_recipe) |>\n",
    "               add_model(knn_tune) |>\n",
    "               tune_grid(resamples = heart_vfold, grid = 10) |>\n",
    "               collect_metrics()\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae4b8d-09b4-4e77-b6f7-49bd7985b0bb",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20d19-7d4a-4f52-972d-d84213df81ab",
   "metadata": {},
   "source": [
    "Now we find the best value of K.\n",
    "\n",
    "We first filter for accuracy from the result of the workflow analysis we just performed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6ecde8-ca34-4d67-9bd6-b5e473ab3652",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a16bb-7184-48d7-bf8e-81dfffd28406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracies <- knn_results |>\n",
    "              filter(.metric == \"accuracy\")\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e480464-b118-4f3e-886d-c8655e09991b",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72266067-07ca-450f-9976-4420109a27b0",
   "metadata": {},
   "source": [
    "Then we create a line plot\n",
    "\n",
    "using the accuracies dataset we just created\n",
    "\n",
    "with K-values on the x-axis\n",
    "\n",
    "and the mean on the y-axis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd96a5d-5ec2-463d-af15-006a16ff5948",
   "metadata": {},
   "source": [
    "Source for geom_label: \n",
    "\n",
    "https://stackoverflow.com/questions/44741127/r-ggplot2-how-to-write-the-y-coordinate-of-every-data-point-right-next-to-it\n",
    "\n",
    "https://stackoverflow.com/questions/7263849/what-do-hjust-and-vjust-do-when-making-a-plot-using-ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e820b4b-9c93-4da0-881a-0ce31a8734eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_versus_k <- ggplot(accuracies, aes(x = neighbors, y = mean)) +\n",
    "                     geom_point(aes(neighbors, mean)) +\n",
    "                     geom_line(aes(neighbors, mean)) +\n",
    "                     geom_label(aes(label=mean), hjust=c(0,0,0,0,0.4,0,0,0,0.6,0.5), vjust=c(1.5,-0.5,1.5,1.5,-0.7,1.3,-0.9,1.7,-0.4,1.8)) +\n",
    "                     labs(x = \"K-value (Number of Neighbours)\", y = \"Accuracy Estimate\") +\n",
    "                     scale_x_continuous(breaks = seq(0, 14, by = 1)) +\n",
    "                     scale_y_continuous(limits = c(0.4, 1.0))\n",
    "accuracy_versus_k\n",
    "\n",
    "# geom_label(aes(label=mean)) +\n",
    "# , hjust=c(0,0,0,0,0,-1,1,0,0,0), vjust=c(0,0,0,0,0,-1,1,0,0,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1b304-5b15-4112-8d3f-c40cd1986416",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa2995-a150-4d18-816b-e6e72d1f7c78",
   "metadata": {
    "tags": []
   },
   "source": [
    "The line plot suggests that 10 is the value of K that provides the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f74419-34a7-4bc5-a8a5-206df8e516bf",
   "metadata": {},
   "source": [
    "Note for team:\n",
    "\n",
    "Alternatively, we could've imputed the missing entries for numerical variables in heart_data and called prep() and bake() on it to create a brand new dataset before we split heart_data into a training set and a testing set.\n",
    "\n",
    "However, nowhere in the textbook nor the worksheets/tutorials do they do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc096b-5972-41e8-9336-69bc3a6490c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d16d5-597b-44ae-9093-a1340d7dfe78",
   "metadata": {},
   "source": [
    "Now that we have applied cross validation to find the best k value, we can build our final model. We first created the new model specification with the best K value and then passed the model specification and training data into a workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b7cce-441b-4373-acb4-ce6777641976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_spec_best <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 10) |>\n",
    "       set_engine(\"kknn\") |>\n",
    "       set_mode(\"classification\")\n",
    "\n",
    "heart_fit_best <- workflow() |>\n",
    "               add_recipe(heart_recipe) |>\n",
    "               add_model(knn_spec_best)|> \n",
    "               fit(data = heart_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40de7a-400e-4f91-829b-b2b7df07be89",
   "metadata": {},
   "source": [
    "The accuracy of the predictions on the testing data were then explored using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f91d67-1fd3-447a-aeb1-95daa019fb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_test_predictions_best <- predict(heart_fit_best, heart_test) |> \n",
    "    bind_cols(heart_test)\n",
    "\n",
    "heart_model_metrics_best <- heart_test_predictions_best |> \n",
    "        metrics(truth = Class, estimate = .pred_class)\n",
    "\n",
    "heart_mat_best <- heart_test_predictions_best |> \n",
    "    conf_mat(truth = Class, estimate = .pred_class)\n",
    "heart_mat_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81871065-f879-46e0-8a8b-8e56f33bf1d6",
   "metadata": {},
   "source": [
    "**ADD TITLE FOR THIS TABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d0cfc-8f1c-4023-bd90-ec05117e6bd3",
   "metadata": {},
   "source": [
    "#### Accuracy and Precision ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650bfc1-8db6-43d1-b08f-3acf40929057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = \"accuracy =\" \n",
    "text2 = \"precision =\"\n",
    "accuracy = (76 + 96)/ (76+32+27+96)\n",
    "precision = 96/ (96+32)\n",
    "print(paste(text1, accuracy)) \n",
    "print(paste(text2, precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bc994-3f88-48ee-a23a-0db6e5244c33",
   "metadata": {},
   "source": [
    "# Discussion #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f50b12-3509-4516-a561-ab392654a64d",
   "metadata": {},
   "source": [
    "To answer our predictive question: \n",
    "\n",
    "*How well can the presence of heart disease be predicted in patients through the classification of ST depression induced by exercise relative to rest and maximum heart rate as key factors in the assessment?*\n",
    "\n",
    "\n",
    "We can look at the accuracy and precision calculated from the confusion matrix: \n",
    "\n",
    "The __accuracy__ is about 74.5%, meaning that the model correctly predicted the outcome for 74.5% of the instances in the data set. \n",
    "\n",
    "The __precision__ is about 75.0%, meaning that out of all the patients predicted with heart disease, about 75.0% of the model truly had heart disease. \n",
    "\n",
    "\n",
    "\n",
    "The model appears to have a reasonably good performance with respect to the confusion metrics calculated. However, given that the problem at hand is predicting the presence of heart disease, the accuracy and precision of the model become particularly important as false positives and false negatives can have serious consequences when predicting if a patient has heart disease. As such, a higher accuracy and precision valuable would be more desirable. \n",
    "\n",
    "\n",
    "This indicates that ST depression induced by exercise relative to rest and maximum heart rate are able to contribute to predicting heart disease. However, to further improve the model's performance in predicting the presence of heart disease, additional risk factors associated with heart disease may be incorporated into the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b016-8207-42df-90e1-c9f246be52c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Expected outcomes and significance #\n",
    "\n",
    "## What do you expect to find? ##\n",
    "This study aims to propose an accurate classification model for heart disease prediction using a machine learning classification algorithm, K-nearest neighbors, that can effectively categorize individuals based on their medical data.\n",
    "\n",
    "## What impact could such findings have? ##\n",
    "The findings of this study could have the potential to make an impact in the field of medical health since an accurate heart disease prediction model can assist intervention measures, which may lead to better patient outcomes.\n",
    "\n",
    "## What future questions could this lead to? ##\n",
    "Future questions that this could raise involve the comparability of the K-nearest neighbors classifier model to other models. <br><br> For example, <br> 1. How does the accuracy of prediction with the K-nearest neighbors approach compare with other prediction models based on different machine learning algorithms?\n",
    "<br>2. What are the advantages and limitations of the K-nearest neighbors method in comparison to the other methods in the case of predicting the heart disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048ab5c-8ad9-4ef7-a8f1-5ef88fbfa099",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Here's a list of stuff we probably won't need anymore:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592668b-e404-47bf-9c8f-fac5596497f2",
   "metadata": {},
   "source": [
    "Before carrying on with the classification process, I am going to clean the non-imputed dataset in an effort to pick our predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293ba9c-0c72-4c2a-8271-721e9b42e6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global_dataset_missing <- global_dataset_missing |>\n",
    "\n",
    "\n",
    "#       mutate(Class = as.factor(Class)) |>\n",
    "#       mutate(Class = fct_recode(Class, \"healthy\" = \"0\", \"sick\" = \"1\", \"sick\" = \"2\", \"sick\" = \"3\", \"sick\" = \"4\")) |>\n",
    "\n",
    "\n",
    "#       mutate(sex = as_factor(sex)) |>\n",
    "#       mutate(sex = fct_recode(sex, \"male\" = \"1\", \"female\" = \"0\")) |>\n",
    "\n",
    "\n",
    "#       mutate(cp = as_factor(cp)) |>\n",
    "#       mutate(cp = fct_recode(cp, \"typical angina\" = \"1\", \"atypical angina\" = \"2\", \"non-anginal pain\" = \"3\", \"asymptomatic\" = \"4\")) |>\n",
    "\n",
    "\n",
    "#       mutate(fbs = as_factor(fbs)) |>\n",
    "#       mutate(fbs = fct_recode(fbs, \"true\" = \"1\", \"false\" = \"0\")) |>      \n",
    "\n",
    "\n",
    "#       mutate(restecg = as_factor(restecg)) |>\n",
    "#       mutate(restecg = fct_recode(restecg, \"normal\" = \"0\", \"ST-T wave abnormality\" = \"1\", \"left ventricular hypertrophy\" = \"2\")) |>\n",
    "\n",
    "\n",
    "#       mutate(exang = as_factor(exang)) |>\n",
    "#       mutate(exang = fct_recode(exang, \"yes\" = \"1\", \"no\" = \"0\")) |>\n",
    "\n",
    "\n",
    "#       mutate(slope = as_factor(slope)) |>\n",
    "#       mutate(slope = fct_recode(slope, \"upsloping\" = \"1\", \"flat\" = \"2\", \"downsloping\" = \"3\")) |>\n",
    "      \n",
    "\n",
    "#       mutate(thal = as_factor(thal)) |>\n",
    "#       mutate(thal = fct_recode(thal, \"normal\" = \"3\", \"fixed\" = \"6\", \"reversable\" = \"7\"))\n",
    "\n",
    "# global_dataset_missing <- global_dataset_missing |>\n",
    "#       rename(Age = age, Sex = sex, \"Chest_Pain_Type\" = cp, \"Resting_Blood_Pressure\" = trestbps, Cholesterol = chol, \"Fasting_blood_sugar_over_120_mg/dl\" = fbs,\n",
    "#       \"Resting_ecg_results\" = restecg, \"Max_heart_rate\" = thalach, \"Exercise_induced_angina\" = exang, \"ST_depression_induced_by_exercise_relative_to_rest\" = oldpeak, \n",
    "#       \"slope_of_the_peak_exercise_ST_segment\" = slope, \"Number_of_major_vessels_colored_by_flourosopy\" = ca, \"Thalassemia\" = thal)\n",
    "\n",
    "# global_dataset_missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
